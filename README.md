# bert-universal-encoder
1. 提供了常见bert的通用字符串到编码的预处理方法
2. 提供了bert的finetune示例

#### 项目入口
- 项目的入口文件是：
```
bert_encoder.py
bert_finetune.py
```

- 项目的函数get_encode，输入是一个字符串，输出是一段编码，编码的大小可以设置
- 项目的函数get_token_mask，输入是一个字符串，输出是字符串的id列表和mask列表，这两个列表可以传入bert模型进行bert训练

如下：
#### get_encode调用的方法如下

```
input_str = '我是一只小可爱'
# pad_size指的是输入的序列的长度的最大值
pad_size = 32
# bert_path指的是存放bert模型的文件夹, 不同的bert模型放入不同的文件夹，只要是切换文件夹就可以切换模型
bert_path = '/opt/app/bert_one_path/'
# 直接实例化模型，并调用get_encode方法即可
te = TokenEncode(bert_path, pad_size)
a, b = te.get_encode(input_str)
print(a)
print(b)
```

输出
```
tensor([[[ 0.4921, -0.4516, -0.4388,  ...,  0.9449, -0.3295, -0.3833],
         [ 1.4511, -0.1205,  0.4555,  ..., -0.9957, -0.1408, -0.0016],
         [ 0.7878, -1.1019,  0.7672,  ..., -0.1514,  0.4297, -0.4050],
         ...,
         [ 0.4307, -0.3707, -0.2189,  ..., -0.0205,  0.1653, -0.4909],
         [ 0.2882, -0.6113, -0.0436,  ...,  0.3526, -0.1981, -0.3887],
         [ 0.3016, -0.3046, -0.1259,  ...,  0.1952,  0.0146, -0.4004]]],
       grad_fn=<AddBackward0>) 

tensor([[ 0.9993, -0.6803,  1.0000, ..., -0.2602, -0.9093, -0.9995,  0.8292]],
       grad_fn=<TanhBackward>)
```

